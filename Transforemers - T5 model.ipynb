{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vinoth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Vinoth\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class handles the tokenization process for T5 models. Tokenization involves converting raw text into numerical representations (tokens) that the model can understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"D:/Data science Notes/Python/Projects/Text summarization/results\",          # output directory for checkpoints\n",
    "    num_train_epochs=6,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=\"D:/Data science Notes/Python/Projects/Text summarization/logs\",            # directory for storing logs\n",
    "    logging_steps=50,                # how often to log training info\n",
    "    save_steps=500,                  # how often to save a model checkpoint\n",
    "    eval_steps=50,                   # how often to run evaluation\n",
    "    eval_strategy=\"epoch\",     # Ensure evaluation happens every `epoch`\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used Arguments:\n",
    "\n",
    "output_dir:\n",
    "    Purpose: Specifies the directory where the trained model, checkpoints, and other training artifacts will be saved.\n",
    "    Value: \"D:/Data science Notes/Python/Projects/Text summarization/results\"\n",
    "\n",
    "num_train_epochs:\n",
    "    Purpose: Determines the total number of times the entire training dataset will be passed through the model during training.\n",
    "    Value: 6 (meaning the model will be trained for 6 epochs)\n",
    "\n",
    "per_device_train_batch_size:\n",
    "    Purpose: Controls the number of samples processed by the model in a single training step on each device (e.g., GPU).\n",
    "    Value: 8 (meaning 8 samples will be processed together in each training step)\n",
    "\n",
    "per_device_eval_batch_size:\n",
    "    Purpose: Similar to per_device_train_batch_size, but for the evaluation phase.\n",
    "    Value: 8\n",
    "\n",
    "warmup_steps:\n",
    "    Purpose: Used with learning rate schedulers. During the warmup phase, the learning rate gradually increases from a very low value to its maximum. This helps stabilize training.\n",
    "    Value: 500 (meaning the learning rate will increase gradually for the first 500 training steps)\n",
    "\n",
    "weight_decay:\n",
    "    Purpose: A regularization technique that penalizes large weights in the model, preventing overfitting.\n",
    "    Value: 0.01 (a small value for weight decay)\n",
    "\n",
    "logging_dir:\n",
    "    Purpose: Specifies the directory where training logs will be saved (e.g., for TensorBoard).\n",
    "    Value: \"D:/Data science Notes/Python/Projects/Text summarization/logs\"\n",
    "\n",
    "logging_steps:\n",
    "    Purpose: Controls how often training information (e.g., loss, learning rate) is logged.\n",
    "    Value: 50 (meaning logs will be printed every 50 training steps)\n",
    "\n",
    "save_steps:\n",
    "    Purpose: Determines how often model checkpoints are saved during training.\n",
    "    Value: 500 (meaning a checkpoint will be saved every 500 training steps)\n",
    "\n",
    "eval_steps:\n",
    "    Purpose: Controls how often the model is evaluated on the validation set during training.\n",
    "    Value: 50 (meaning evaluation will be performed every 50 training steps)\n",
    "\n",
    "eval_strategy:\n",
    "    Purpose: Controls when evaluation should occur.\n",
    "    Value: \"epoch\" (meaning evaluation will be performed at the end of each epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Available Arguments:\n",
    "\n",
    "learning_rate: The initial learning rate for the optimizer.\n",
    "\n",
    "evaluation_strategy: Can also be set to \"steps\" or \"no\".\n",
    "\n",
    "dataloader_num_workers: Number of subprocesses used for data loading.\n",
    "\n",
    "seed: Random seed for reproducibility.\n",
    "\n",
    "fp16: Whether to use mixed precision training.\n",
    "\n",
    "gradient_accumulation_steps: Accumulate gradients over multiple steps before performing an optimizer step.\n",
    "\n",
    "**Many more: Refer to the official Hugging Face Transformers documentation for a complete list of available arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Purpose: This class represents the T5 model architecture specifically designed for conditional text generation.\n",
    "\n",
    "Key Features:\n",
    "\n",
    "Encoder-Decoder Architecture: T5 employs a powerful encoder-decoder structure, where the encoder processes the input sequence and the decoder generates the output sequence.\n",
    "\n",
    "Text-to-Text Framework: T5 treats all NLP tasks as text-to-text problems, such as:\n",
    "\n",
    "Translation: \"translate English to French: The quick brown fox ...\" -> \"La rapide ...\"\n",
    "\n",
    "Summarization: \"summarize: The article discusses ...\" -> \"The article is about ...\"\n",
    "\n",
    "Question Answering: \"question: What is the capital of France? context: ...\" -> \"Paris\"\n",
    "Â  \n",
    "Pre-trained Models: Hugging Face provides pre-trained T5 models (e.g., \"t5-small\", \"t5-base\", \"t5-large\") with varying sizes and capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purpose: This class simplifies the training process for various Transformer models, including T5. It handles many training-related tasks, such as:\n",
    "\n",
    "Data Loading and Preparation: Efficiently loads and prepares the training and evaluation datasets.\n",
    "\n",
    "Training Loop: Manages the training loop, including forward and backward passes, optimization, and logging.\n",
    "\n",
    "Evaluation: Performs evaluation on the validation set during training.\n",
    "\n",
    "Logging and TensorBoard: Logs training metrics (loss, accuracy, etc.) and can optionally log to TensorBoard for visualization.\n",
    "\n",
    "Checkpointing: Saves model checkpoints at regular intervals, allowing you to resume training or fine-tune from a specific point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
